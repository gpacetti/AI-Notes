What is Prompt Engineering?” to one of the aforementioned LLM-based AI.


Prompt engineering refers to the process of designing and refining prompts or instructions given to a language model, such as GPT-3.5, to generate desired outputs. It involves carefully crafting the input text provided to the model to elicit the desired response or behavior.

Prompt engineering can be used to fine-tune the output of language models by specifying explicit instructions, adding context, or using specific formatting techniques to guide the model’s generation. By tweaking the prompts, developers and users can influence the output to be more accurate, coherent, or aligned with a specific goal.

Effective prompt engineering requires an understanding of the underlying language model and its capabilities. It involves experimenting with different prompts, iterating on them, and analyzing the model’s responses to achieve the desired results. Prompt engineering is particularly important when using large language models like GPT-3.5, as their outputs can sometimes be unpredictable or require additional context to generate the desired output.

Overall, prompt engineering is a crucial aspect of working with language models to ensure they produce accurate and meaningful responses that align with the user’s intentions or requirements.

Within the realm of Large Language Models (LLMs) like GPT-3.5, there are a few terms you might come across, namely “zero-shot”, “one-shot”, and “few-shot” prompting. These terminologies refer to the way models are instructed or prompted.

In a “zero-shot” scenario, the model is given a task without prior examples; it must deduce what to do from the prompt and its existing training.

In the next couple of labs, we’ll refer to naive or standard prompting a few times. That is the prompt someone who has yet to use LLM-based AI would use. They’d ask their question without providing any examples for the model.

For example, when we asked, “What is Prompt Engineering?” our prompt was a zero-shot prompt.

On the other hand, “one-shot” provides the model with a single example, while “few-shot” offers several examples to help guide the model’s output. These methods are instrumental when you’re trying to instruct the model in a way that it might not have been explicitly trained on. By providing examples, users can align the model’s thinking to a specific desired output or format.

Chain-of-Thought (CoT), a more advanced prompting technique explored later in the course, is an example of one-shot prompting (or few-shot prompting, depending on how we phrase our prompt).

English as a new programming language, because it is possible to prompt a LLM with a Data Set and have a result that is the same as a programming code can do. For istance we can input a list of score to sort out. The result is the list sorted out, we do not need code for that.

Here are 10 people who have made significant contributions to the development of Language Models (LLMs):

1. Geoffrey Hinton: A renowned computer scientist and one of the pioneers of deep learning, Hinton has made significant contributions to the development of LLMs, including his work on recurrent neural networks (RNNs) and transformer models.

2. Yoshua Bengio: Another prominent figure in the field of deep learning, Bengio has contributed to the development of LLMs through his research on neural networks, particularly in the areas of unsupervised learning and generative models.

3. Yann LeCun: LeCun is a computer scientist and one of the key contributors to the development of convolutional neural networks (CNNs), which have been instrumental in advancing LLMs, especially in the field of natural language processing.

4. Ilya Sutskever: As one of the co-founders of OpenAI, Sutskever has played a crucial role in the development of LLMs, particularly through his work on sequence-to-sequence models and the attention mechanism.

5. Oriol Vinyals: Vinyals is a research scientist at Google DeepMind and has made notable contributions to LLMs, including his work on generative models and reinforcement learning.

6. Kyunghyun Cho: Cho is a professor at New York University and has made significant contributions to the development of LLMs, particularly through his research on neural machine translation and attention mechanisms.

7. Andrej Karpathy: Karpathy is the Director of AI at Tesla and has contributed to the advancement of LLMs through his research on image captioning and the use of recurrent neural networks for natural language generation.

8. Alex Graves: Graves is a researcher known for his work on recurrent neural networks and sequence generation. His contributions to LLMs include his research on handwriting generation and speech recognition.

9. Quoc Le: Le is a research scientist at Google and has made notable contributions to LLMs, including his work on the development of the Transformer model and its applications in natural language understanding.

10. Thomas Wolf: Wolf is a researcher at Hugging Face, a company specializing in natural language processing, and has contributed to the development of LLMs, particularly through his work on the popular language model "BERT" (Bidirectional Encoder Representations from Transformers).