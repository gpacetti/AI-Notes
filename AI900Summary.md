

Clustering is a form of machine learning that is used to group similar items into clusters based on their features. For example, a researcher might take measurements of penguins, and group them based on similarities in their proportions.

Graphic of penguins clustered into groups.

Clustering is an example of unsupervised machine learning, in which you train a model to separate items into clusters based purely on their characteristics, or features. There is no previously known cluster value (or label) from which to train the model.

The K-Means Clustering algorithm groups items into the number of clusters, or centroids, you specify - a value referred to as K.

The K-Means algorithm works by:

Initializing K coordinates as randomly selected points called centroids in n-dimensional space (where n is the number of dimensions in the feature vectors).
Plotting the feature vectors as points in the same space, and assigning each point to its closest centroid.
Moving the centroids to the middle of the points allocated to it (based on the mean distance).
Reassigning the points to their closest centroid after the move.
Repeating steps 3 and 4 until the cluster allocations stabilize or the specified number of iterations has completed.

https://microsoftlearning.github.io/AI-900-AIFundamentals/instructions/02c-create-clustering-model.html

Note You can think of data observations, like the penguin measurements, as being multidimensional vectors. The K-Means algorithm works by:

initializing K coordinates as randomly selected points called centroids in n-dimensional space (where n is the number of dimensions in the feature vectors).
Plotting the feature vectors as points in the same space, and assigning each point to its closest centroid.
Moving the centroids to the middle of the points allocated to it (based on the mean distance).
Reassigning the points to their closest centroid after the move.
Repeating steps c. and d. until the cluster allocations stabilize or the specified number of iterations has completed.
--------------------------------------

As of July 2023, Azure AI services encompass all of what were previously known as Cognitive Services and Azure Applied AI Services 1. There are no changes to pricing 1. The names Cognitive Services and Azure Applied AI continue to be used in Azure billing, cost analysis, price list, and price APIs 1.
Computer vision is one of the core areas of artificial intelligence (AI), and focuses on creating solutions that enable AI applications to "see" the world and make sense of it.

Of course, computers don't have biological eyes that work the way ours do, but they are capable of processing images; either from a live camera feed or from digital photographs or videos. This ability to process images is the key to creating software that can emulate human visual perception.

Some potential uses for computer vision include:

Content Organization: Identify people or objects in photos and organize them based on that identification. Photo recognition applications like this are commonly used in photo storage and social media applications.
Text Extraction: Analyze images and PDF documents that contain text and extract the text into a structured format.
Spatial Analysis: Identify people or objects, such as cars, in a space and map their movement within that space.
To an AI application, an image is just an array of pixel values. These numeric values can be used as features to train machine learning models that make predictions about the image and its contents.

Microsoft's Azure AI Vision service provides pre-built computer vision capabilities. The service can analyze images, and return detailed information about an image and the objects it depicts.

To use Azure AI Vision, you need to create a resource for it in your Azure subscription. You can use either of the following resource types:

Computer Vision: A specific resource for the Computer Vision service. Use this resource type if you don't intend to use any other Azure AI Services, or if you want to track utilization and costs for your Computer Vision resource separately.
Azure AI services: A general resource that includes Computer Vision along with many other Azure AI services; such as Azure AI Language, Azure AI Custom Vision, Azure AI Translator, and others. Use this resource type if you plan to use multiple AI services and want to simplify administration and development.

Tagging visual features
The image descriptions generated by Azure AI Vision are based on a set of thousands of recognizable objects, which can be used to suggest tags for the image. These tags can be associated with the image as metadata that summarizes attributes of the image; and can be particularly useful if you want to index an image along with a set of key terms that might be used to search for images with specific attributes or contents.

Detecting objects
The object detection capability is similar to tagging, in that the service can identify common objects; but rather than tagging, or providing tags for the recognized objects only, this service can also return what is known as bounding box coordinates. Not only will you get the type of object, but you will also receive a set of coordinates that indicate the top, left, width, and height of the object detected, which you can use to identify the location of the object in the image.

Detecting brands
This feature provides the ability to identify commercial brands. Azure AI Vision has an existing database of thousands of globally recognized logos from commercial brands of products.

Detecting faces
Azure AI Vision can detect and analyze human faces in an image, including the ability to determine age and a bounding box rectangle for the location of the face(s). The facial analysis capabilities of Azure AI Vision service are a subset of those provided by the dedicated Azure AI Face service.

Categorizing an image
Azure AI Vision can categorize images based on their contents. The service uses a parent/child hierarchy with a "current" limited set of categories. When analyzing an image, detected objects are compared to the existing categories to determine the best way to provide the categorization. As an example, one of the parent categories is people_. This image of a person on a roof is assigned a category of people_.

Detecting domain-specific content -
When categorizing an image, the Azure AI Vision service supports two specialized domain models:

**Celebrities** - The service includes a model that has been trained to identify thousands of well-known celebrities from the worlds of sports, entertainment, and business.
**Landmarks** - The service can identify famous landmarks, such as the Taj Mahal and the Statue of Liberty.


**Optical character recognition**
Azure AI Vision service can use optical character recognition (OCR) capabilities to detect printed and handwritten text in images. 

In addition to these capabilities, the Azure AI Vision service can:

**Detect image types** - for example, identifying clip art images or line drawings.
Detect image color schemes - specifically, identifying the dominant foreground, background, and overall colors in an image.
Generate thumbnails - creating small versions of images.
Moderate content - detecting images that contain adult content or depict violent, gory scenes.

https://microsoftlearning.github.io/AI-900-AIFundamentals/instructions/03-module-03.html

Azure's Azure AI Vision service gives you access to advanced algorithms that process images and return information based on the visual features you're interested in.

![Alt text](image.png)

https://portal.vision.cognitive.azure.com/gallery/featured

Image classification is a common workload in artificial intelligence (AI) applications. It harnesses the predictive power of machine learning to enable AI systems to identify real-world items based on images.

Most modern image classification solutions are based on deep learning techniques that make use of convolutional neural networks (CNNs) to uncover patterns in the pixels that correspond to particular classes. Training an effective CNN is a complex task that requires considerable expertise in data science and machine learning.

Common techniques used to train image classification models have been encapsulated into the Azure AI Custom Vision service in Microsoft Azure; making it easy to train a model and publish it as a software service with minimal knowledge of deep learning techniques. You can use the Azure AI Custom Vision to train image classification models and deploy them as services for applications to use.

**Face detection** and analysis is an area of artificial intelligence (AI) in which we use algorithms to locate and analyze human faces in images or video content.

**Facial analysis**
Moving beyond simple face detection, some algorithms can also return other information, such as facial landmarks (nose, eyes, eyebrows, lips, and others).
These facial landmarks can be used as features with which to train a machine learning model.

A further application of facial analysis is to train a machine learning model to identify known individuals from their facial features. This usage is more generally known as **facial recognition**, and involves using multiple images of each person you want to recognize to train a model so that it can detect those individuals in new images on which it wasn't trained.

Microsoft Azure provides multiple Azure AI services that you can use to detect and analyze faces, including:

Azure AI Computer Vision, which offers face detection and some basic face analysis, such as returning the bounding box coordinates around an image.
Azure AI Video Indexer, which you can use to detect and identify faces in a video.
Azure AI Face, which offers pre-built algorithms that can detect, recognize, and analyze faces.

Effective today, new customers need to apply for access to use facial recognition operations in Azure Face API, Computer Vision, and Video Indexer. Existing customers have one year to apply and receive approval for continued access to the facial recognition services based on their provided use cases. By introducing Limited Access, we add an additional layer of scrutiny to the use and deployment of facial recognition to ensure use of these services aligns with Microsoft’s Responsible AI Standard and contributes to high-value end-user and societal benefit. This includes introducing use case and customer eligibility requirements to gain access to these services. Read about example use cases, and use cases to avoid, here. Starting June 30, 2023, existing customers will no longer be able to access facial recognition capabilities if their facial recognition application has not been approved. Submit an application form for facial and celebrity recognition operations in Face API, Computer Vision, and Azure Video Indexer here, and our team will be in touch via email.
https://aka.ms/facerecognition
![Alt text](image-1.png)

**Optical character recognition (OCR)** enables artificial intelligence (AI) systems to read text in images, enabling applications to extract information from photographs, scanned documents, and other sources of digitized text, where computer vision intersects with natural language processing. You need computer vision capabilities to "read" the text, and then you need natural language processing capabilities to make sense of it.
The **Read API** can handle scanned documents that have a lot of text. It also has the ability to automatically determine the proper recognition model to use, taking into consideration lines of text and supporting images with printed text as well as recognizing handwriting.
**Azure AI Document Intelligence** service can solve for this issue by digitizing fields from documents using optical character recognition (OCR). 
Azure AI Document Intelligence supports automated document processing through:
**Prebuilt models** that are trained to recognize and extract data for common scenarios such as IDs, receipts, and invoices.
**Custom models**, which enable you to extract what are known as key/value pairs and table data from forms. Custom models are trained using your own data, which helps to tailor this model to your specific forms. Starting with a few samples of your forms, you can train the custom model. After the first training exercise, you can evaluate the results and consider if you need to add more samples and re-train.

**Azure AI Language** service can help simplify application development by using pre-trained models that can:

Determine the language of a document or text (for example, French or English).
Perform sentiment analysis on text to determine a positive or negative sentiment.
Extract key phrases from text that might indicate its main talking points.
Identify and categorize entities in the text. Entities can be people, places, organizations, or even everyday items such as dates, times, quantities, and so on.

Statistical analysis of terms used in the text. For example, removing common **"stop words"** (words like "the" or "a", which reveal little semantic information about the text), and performing frequency analysis of the remaining words (counting how often each word appears) can provide clues about the main subject of the text.
Extending frequency analysis to multi-term phrases, commonly known as **N-grams** (a two-word phrase is a bi-gram, a three-word phrase is a tri-gram, and so on).
Applying **stemming or lemmatization** algorithms to normalize words before counting them - for example, so that words like "power", "powered", and "powerful" are interpreted as being the same word.
Applying linguistic structure rules to analyze sentences - for example, breaking down sentences into tree-like structures such as a **noun phrase**, which itself contains nouns, verbs, adjectives, and so on.
Encoding words or terms as numeric features that can be used to train a machine learning model. For example, to classify a text document based on the terms it contains. This technique is often used to perform **sentiment analysis**, in which a document is classified as positive or negative.
Creating **vectorized models** that capture semantic relationships between words by assigning them to locations in n-dimensional space. This modeling technique might, for example, assign values to the words "flower" and "plant" that locate them close to one another, while "skateboard" might be given a value that positions it much further away.

Azure AI Language to analyze the text ":-)", results in a value of unknown for the language name and the language identifier, and a score of NaN (which is used to indicate not a number).

Language detection capability of Azure AI Language to identify the language in which text is written.

Azure AI Language uses a prebuilt machine learning classification model to evaluate the text. The service returns a sentiment score in the range of 0 to 1. Values closer to 1 represent a positive sentiment. Scores that are close to the middle of the range (0.5) are considered neutral or indeterminate.

Key phrase extraction is the concept of evaluating the text of a document, or documents, and then identifying the main talking points of the document(s). 

Entity recognition provides Azure AI Language with unstructured text and it will return a list of entities in the text that it recognizes.
-------------------------------------------------------------------------------------------
AI system must support two capabilities:

Speech recognition - the ability to detect and interpret spoken input.
Speech synthesis - the ability to generate spoken output.

 **Azure AI Speech** service, which includes the following application programming interfaces (APIs):

- The Speech to text API
- The Text to speech API

Speech recognition is concerned with taking the spoken word and converting it into data that can be processed - often by transcribing it into a text representation. The spoken words can be in the form of a recorded voice in an audio file, or live audio from a microphone. Speech patterns are analyzed in the audio to determine recognizable patterns that are mapped to words. To accomplish this feat, the software typically uses multiple types of models, including:

An acoustic model that converts the audio signal into phonemes (representations of specific sounds).
A language model that maps phonemes to words, usually using a statistical algorithm that predicts the most probable sequence of words based on the phonemes.

Speech synthesis is in many respects the reverse of speech recognition. It is concerned with vocalizing data, usually by converting text to speech. A speech synthesis solution typically requires the following information:

The text to be spoken.
The voice to be used to vocalize the speech.
To synthesize speech, the system typically tokenizes the text to break it down into individual words, and assigns phonetic sounds to each word. It then breaks the phonetic transcription into prosodic units (such as phrases, clauses, or sentences) to create phonemes that will be converted to audio format. These phonemes are then synthesized as audio by applying a voice, which will determine parameters such as pitch and timbre; and generating an audio wave form that can be output to a speaker or written to a file.

The service includes multiple pre-defined voices with support for multiple languages and regional pronunciation, including standard voices as well as neural voices that leverage neural networks to overcome common limitations in speech synthesis with regard to intonation, resulting in a more natural sounding voice. You can also develop custom voices and use them with the text to speech API

Text translation can be used to translate documents from one language to another, translate email communications that come from foreign governments, and even provide the ability to translate web pages on the Internet. Many times you will see a Translate option for posts on social media sites, or the Bing search engine can offer to translate entire web pages that are returned in search results.
Azure AI Translator uses a Neural Machine Translation (NMT) model for translation, which analyzes the semantic context of the text and renders a more accurate and complete translation as a result.
Azure AI Translator supports text-to-text translation between more than 60 languages. When using the service, you must specify the language you are translating from and the language you are translating to using ISO 639-1 language codes, such as en for English, fr for French, and zh for Chinese. Alternatively, you can specify cultural variants of languages by extending the language code with the appropriate 3166-1 cultural code - for example, en-US for US English, en-GB for British English, or fr-CA for Canadian French.

Speech translation is used to translate between spoken languages, sometimes directly (speech-to-speech translation) and sometimes by translating to an intermediary text format (speech-to-text translation).
---------------------------------------------------------------------------------------------------
**Azure AI Language** service works with conversational language understanding, you need to take into account three core concepts: utterances, entities, and intents (ie. Turing test).

**Utterance** is an example of something a user might say, and which your application must interpret. For example, when using a home automation system, a user might use the following utterances:

"Switch the fan on."

"Turn on the light."

**Entity** is an item to which an utterance refers. For example, fan and light in the following utterances:

"Switch the fan on."

"Turn on the light."

**Intent** represents the purpose, or goal, expressed in a user's utterance. For example, for both of the previously considered utterances, the intent is to turn a device on; so in your conversational language understanding application, you might define a TurnOn intent that is related to these utterances.

 Of special interest is the None intent. You should consider always using the None intent to help handle utterances that do not map any of the utterances you have entered. The None intent is considered a fallback, and is typically used to provide a generic response to users when their requests don't match any other intent.